{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5a5e4f",
   "metadata": {},
   "source": [
    "# Genrating Training CSVs\n",
    "\n",
    "這個程式的功能是從相關article把文字擷取出來，並將文字與rating放到csv檔中，方便之後用pytorch model訓練\n",
    "\n",
    "#### 文字篩選步驟:\n",
    "1. 去除含有Link:, JavaScript...等完全沒用的句子\n",
    "2. 捨棄長度小於10的句子\n",
    "3. 將stop words從句子裡移除\n",
    "4. 計算每個字出現的次數，之後將文字由出現最多的開始加入set中，直到沒有出現次數多於3的文字或set裡含有2000個字為止\n",
    "5. 將set裡的字全部輸入csv檔當作用來訓練的句子\n",
    "\n",
    "#### 檔案位置\n",
    "把這個檔案放在跟train.json, valid.json, test.json同層就能使用了，csv檔會輸出於同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cf53069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 16894/16894 [33:53<00:00,  8.31it/s]\n"
     ]
    }
   ],
   "source": [
    "#generate train,valid set\n",
    "import json\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "#sentences contains these words are useless\n",
    "trash = ['link:','*','javascript','twitter','____','[img]','','\\\\']\n",
    "\n",
    "#stopwords to be cleared\n",
    "stp = stopwords.words('english')\n",
    "stp = stp+['','The','the']\n",
    "\n",
    "\n",
    "\n",
    "with open('train.json', 'r') as f:\n",
    "    with open('top2000train.csv', 'w', newline='',encoding=\"utf-8\") as file:\n",
    "        text = \"\"\n",
    "        writer = csv.writer(file,delimiter=',')\n",
    "        data = json.load(f)\n",
    "        for i in tqdm(data):\n",
    "            wds = []\n",
    "            idx+=1\n",
    "            for j in i['metadata']['premise_articles']:\n",
    "                jsn = i['metadata']['premise_articles'][j]\n",
    "                with open('./articles/'+jsn,'r') as f2:\n",
    "                    txt = json.load(f2)\n",
    "                    #get all words in the article\n",
    "                    for k in txt:\n",
    "                        kk = k.lower()\n",
    "                        kk = kk.split(' ')#tokenize\n",
    "                        if set(kk) & set(trash): #if sentence contains trash words, discard it\n",
    "                            continue\n",
    "                        if len(kk) < 10: #if sentence is not long enough, discard it\n",
    "                            continue\n",
    "                        else:\n",
    "                            #add words into list\n",
    "                            kk = list(filter(str.isalpha,kk))\n",
    "                            kk = list(set(kk).difference(set(stp)))\n",
    "                            wds = wds+kk\n",
    "            #bag of words like processing\n",
    "            bag = dict()\n",
    "            \n",
    "            #count of each words\n",
    "            for j in wds:\n",
    "                if j in bag:\n",
    "                    bag[j] += 1\n",
    "                else:\n",
    "                    bag[j] = 1\n",
    "            #newdict, sort by counts (big to small)\n",
    "            nd={k: v for k, v in sorted(bag.items(), key=lambda item: item[1], reverse=True)}#nlogn\n",
    "            #set of words in article, upmost 2000 words, add words from max count to min count\n",
    "            artset = []\n",
    "            for j in nd:\n",
    "                if len(artset) >= 2000:\n",
    "                    break\n",
    "                if bag[j] >= 3:#only words with count more than 3 will be used\n",
    "                    artset.append(j)\n",
    "                else:\n",
    "                    break\n",
    "            artset.append('endblock')\n",
    "            artset = ' '.join(artset)\n",
    "            #append into csv\n",
    "            writer.writerow([artset,str(i['label']['rating'])])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aca8d052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2360/2360 [04:08<00:00,  9.48it/s]\n"
     ]
    }
   ],
   "source": [
    "#same process as above to generate test set\n",
    "import json\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "idx = 0\n",
    "trash = ['link:','*','javascript','twitter','____','[img]','','\\\\']\n",
    "stp = stopwords.words('english')\n",
    "stp = stp+['','the']\n",
    "\n",
    "\n",
    "\n",
    "with open('test.json', 'r') as f:\n",
    "    with open('top2000test.csv', 'w', newline='',encoding=\"utf-8\") as file:\n",
    "        text = \"\"\n",
    "        writer = csv.writer(file,delimiter=',')\n",
    "        data = json.load(f)\n",
    "        for i in tqdm(data):\n",
    "            wds = []\n",
    "            idx+=1\n",
    "            idd=i['metadata']['id']\n",
    "            for j in i['metadata']['premise_articles']:\n",
    "                jsn = i['metadata']['premise_articles'][j]\n",
    "                with open('./articles/'+jsn,'r') as f2:\n",
    "                    txt = json.load(f2)\n",
    "                    for k in txt:\n",
    "                        kk = k.lower()\n",
    "                        kk = kk.split(' ')\n",
    "                        if set(kk) & set(trash):\n",
    "                            continue\n",
    "                        if len(kk) < 10:\n",
    "                            continue\n",
    "                        else:\n",
    "                            kk = list(filter(str.isalpha,kk))\n",
    "                            kk = list(set(kk).difference(set(stp)))\n",
    "                            wds = wds+kk\n",
    "            bag = dict()\n",
    "            \n",
    "            for j in wds:#n\n",
    "                if j in bag:\n",
    "                    bag[j] += 1\n",
    "                else:\n",
    "                    bag[j] = 1\n",
    "            nd={k: v for k, v in sorted(bag.items(), key=lambda item: item[1], reverse=True)}#nlogn\n",
    "            artset = []\n",
    "            for j in nd:#n\n",
    "                if len(artset) >= 2000:\n",
    "                    break\n",
    "                if bag[j] >= 3:\n",
    "                    artset.append(j)\n",
    "                else:\n",
    "                    break\n",
    "            artset.append('endblock')\n",
    "            artset = ' '.join(artset)\n",
    "            writer.writerow([artset,idd])\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
